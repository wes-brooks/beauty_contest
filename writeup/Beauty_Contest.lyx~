#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Beauty Contest
\end_layout

\begin_layout Abstract
Pithy, concise and informative.
 May bring the reader to tears due to the beauty of it.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The availability of large data sets for building regression models to predict
 the bacterial counts in beach water is both an opportunity and a challenge.
 
\end_layout

\begin_layout Subsection
Data Sources
\end_layout

\begin_layout Standard

\emph on
Possibly move this to the end of the section
\end_layout

\begin_layout Standard
Which sites
\end_layout

\begin_layout Standard
Where are they
\end_layout

\begin_layout Standard
What specific sources sources of data (plug EnDDAT)
\end_layout

\begin_layout Standard
Will include a map and tables
\end_layout

\begin_layout Subsection
Listing of specific statistical techniques
\end_layout

\begin_layout Standard
Fourteen different regression modeling techniques were considered.
 Each technique uses one of five modeling algorithms: GBM, the adaptive
 lasso, the genetic algorithm, PLS, or sparse PLS.
 Each technique is aplied to either continuous or binary regression and
 to either modeling, or variable selection only.
\end_layout

\begin_layout Subsubsection*
Continuous or binary regression
\end_layout

\begin_layout Standard
The goal of predicting exceednaces of the water quality standard was approached
 in two ways: one was to predict the bacterial concentration and then compre
 the prediction to a threshold.
 The other was to predict the state of a binary indicator, which is coded
 as zero when the concetration is below the standard and one when the concentrat
ion exceeds the standard.
 Techniques taking the former approach are continuous regression techniques,
 those taking the latter approach are binary regression techniques.
\end_layout

\begin_layout Subsubsection*
Weighting of observations in binary regression
\end_layout

\begin_layout Standard
A weighting scheme was implemented for some of the binary regression techniques.
 In the weighting scheme, observations were given weights equal to the number
 of standard deviations the observed concentration was from the regulatory
 threshold of 235 CFU/100 mL.
 Any technique that was implemented with this weighting scheme was separately
 implemented without any weighting of the observations.
 The techniques are then labeled weighted and unweighted, respectively.
\end_layout

\begin_layout Subsubsection*
Modeling or selection only
\end_layout

\begin_layout Standard
Some methods are labeled 
\begin_inset Quotes eld
\end_inset

select
\begin_inset Quotes erd
\end_inset

, which means that they are used for variable selection only.
 In these cases, once the variables are selected, the regression model is
 estimated using ordinary least squares for the continuous regression techniques
, or ordinary logistic regression for the binary regression techniques.
\end_layout

\begin_layout Subsubsection
GBM
\end_layout

\begin_layout Standard
GBM refers to the gradient boosting machine (GBM) of 
\begin_inset CommandInset citation
LatexCommand citet
key "Friedman-2001"

\end_inset

.
 A GBM model is a so-called random forest model - a collection of many regressio
n trees.
 Prediction is done by averaging the outputs of the trees.
 Two GBM-based techniques are explored - we refer to them as GBM and GBMCV.
 The difference is in how the optimal number of trees is determined - GBMCV
 selects the number of trees in a model using leave-one-out CV, while GBM
 uses the so-called out-of-bag (OOB) error estimate.
 The CV method is much slower (it has to construct as many random forests
 as there are observations, while the OOB method only requires computing
 a single random forest) but GBMCV should more accurately estimate the predictio
n error.
 All the GBM and GBMCV models share the following settings:
\end_layout

\begin_layout Standard
Number of trees: 10000
\end_layout

\begin_layout Standard
Shrinkage parameter: 0.0005
\end_layout

\begin_layout Standard
Minimum observations per node: 5
\end_layout

\begin_layout Standard
Depth of each tree: 5
\end_layout

\begin_layout Standard
Bagging fraction: 0.5
\end_layout

\begin_layout Subsubsection
Adaptive Lasso
\end_layout

\begin_layout Standard
The adaptive lasso 
\begin_inset CommandInset citation
LatexCommand citet
key "Zou-2006"

\end_inset

 is a regression method that simultaneously selects relevant predictors
 and estimates their coefficients by adding a penalty to the sum of the
 squared residuals.
 For linear regression, the adaptive lasso estimates 
\begin_inset Formula $\hat{{\bm{{\beta}}}}$
\end_inset

 minimize the criterion 
\begin_inset Formula $\sum_{i=1}^{n}(y_{i}-X_{i}\beta)^{2}+\lambda\sum_{j=1}^{p}\frac{{|\beta_{j}|}}{\tilde{{|\beta_{j}|^{\gamma}}}}$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is a tuning parameter and 
\begin_inset Formula $\tilde{{\bm{{\beta}}}}$
\end_inset

 is a consistent estimate of the regression coefficients.
\end_layout

\begin_layout Standard
In this work, 
\begin_inset Formula $\gamma$
\end_inset

 is set to one, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tilde{{\bm{{\beta}}}}$
\end_inset

 are estimated individually by a univariate linear or logistic regression
 (it is necessary to estimate the coefficients individually because there
 are usually more covariates than observations)
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 and the adaptive lasso tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 is selected to minimize the AICc 
\begin_inset CommandInset citation
LatexCommand citep
key "Hurvich-Simonoff-Tsai-1998"

\end_inset

.
\end_layout

\begin_layout Standard
Three of the modeling techniques were based on the adaptive lasso - one
 fo
\end_layout

\begin_layout Subsubsection
Genetic algorithm
\end_layout

\begin_layout Standard
The genetic algorithm 
\begin_inset CommandInset citation
LatexCommand citep
key "Fogel-1998"

\end_inset

 is a variable-selection method that works by analogy to natural selection,
 where so-called chromosomes represent regression models.
 A variable is included in the model if the corresponding element of the
 chromosome is one, but not otherwise.
 Chromosomes are produced in successive generations, where the first generation
 is produced randomly and subsequent generations are produced by combining
 chromosomes from the current generation, with additional random drift.
 The chance that a chromosome in the current generation will produce offspring
 in the next generation is an increasing function of its fitness.
 The fitness of each chromosome is calculated by the corrected Akaike Informatio
n Criterion (AICc) 
\begin_inset CommandInset citation
LatexCommand citet
key "Akaike-1973,Hurvich-Tsai-1989"

\end_inset

.
\end_layout

\begin_layout Standard
The implementations in this study used 100 generations, with each generation
 consisting of 200 chromosomes.
 The genetic algorithm method GALM is the default for linear regression
 modeling in Virtual Beach 
\begin_inset CommandInset citation
LatexCommand citep
key "Cyterski-Brooks-Galvin-Wolfe-Carvin-Roddick-Fienen-Corsi-2013"

\end_inset

.
 The study also investigates two genetic algorithm methods for logistic
 regression: one weighted (GALogistic-weighted) and one unweighted (GALogistic-u
nweighted).
\end_layout

\begin_layout Subsubsection
PLS
\end_layout

\begin_layout Standard
Partial least squares (PLS) regression is a tool for building regression
 models with many covariates 
\begin_inset CommandInset citation
LatexCommand citep
key "Wold-Sjostrum-Eriksson-2001"

\end_inset

.
 PLS works by decomposing the covariates into mutually orthogonal components,
 with the components then used as the variables in a regression model.
 This is similar to principal components regression (PCR), but the way PLS
 components are chosen ensures that they are aligned with the model output.
 On the other hand, PCR is sometimes criticised for decomposing the covariates
 into components that are unrelated to the model's output.
\end_layout

\begin_layout Standard
To use PLS, one must decide how many components to use in the model.
 The technique used in this study follows the method described in 
\begin_inset CommandInset citation
LatexCommand citet
key "Brooks-Fienen-Corsi-2013"

\end_inset

, using the PRESS statistic to select the number of components.
\end_layout

\begin_layout Subsubsection
SPLS
\end_layout

\begin_layout Standard
Sparse PLS (SPLS) is introduced in 
\begin_inset CommandInset citation
LatexCommand citet
key "Chun-Keles-2007"

\end_inset

.
 The SPLS method combines the orthogonal decompositions of PLS with the
 sparsity of lasso-type variable selection.
 To do so, SPLS uses two tuning parameters: one that controls the number
 of orthogonal components and one that controls the lasso-type penalty.
 The optimal parameters are those that minimize the mean squared prediction
 error (MSEP) over a two-dimensional grid search.
 The MSEP is calculated by 10-fold cross-validation.
\end_layout

\begin_layout Subsection
Implementation for beach regression
\end_layout

\begin_layout Standard
The response variable for our continuous regression models is the natural
 logarithm of the E.
 coli concentration.
 For the binary regression models, the response variable is 
\end_layout

\begin_layout Standard
Include a table with pre/post processing discussion
\end_layout

\begin_layout Standard
This includes tuning of parameters
\end_layout

\begin_layout Standard
Some specific data issues because we are estimating a threshold exceedence
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
Assessment of the modeling techniques is based on their performance in predictin
g exceedances of the regulatory standard.
 Two types of cross validation was used to measure the performance in prediction
: leave-one-out (LOO) and leave-one-year-out (LOYO).
 In LOO CV, one observation is held out for validation while the rest of
 the data (the model data) is used to train a model.
 The model is used to predict the concentration of that held out observation,
 and the process is repeated for each observation.
 Each cycle of LOYO CV holds out one year's worth of data for validation
 instead of a single observation.
 It is intended to approximate the performance of the modeling technique
 under a typical use case: a new model is estimated before the start of
 each annual beach season and then used for predicting exceedances during
 the season.
 That year's data is then added to the dataset to estimate a model for the
 next beach season.
 The LOYO models in this study were estimated using all the available data,
 even that from future years - so for instance the 2012 models were estimated
 using the 2010-2011 and 2013 data
\end_layout

\begin_layout Standard
Some methods also used cross-validation internally to select tuning parameters.
 In those cases the internal CV was done by partitioning the model data,
 leaving out one partition at a time.
 This process is separate from - and does not affect - the CV to assess
 predictive performance.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
How did we evaluate the performance of each technique on all the different
 data sets
\end_layout

\begin_layout Itemize
for all cases ---> AUC (ROC curve)
\end_layout

\begin_layout Itemize
continuous variables using PRESS (skill --> Like Nash-Sutcliffe/R^2 over
 the fitted data) 
\end_layout

\begin_layout Itemize
True/False Positives/Negatives (needs a threshold)
\end_layout

\begin_layout Itemize
Which variables are selected for models where variable reduction takes place
\end_layout

\begin_deeper
\begin_layout Itemize
challenge regarding the fact that different variables are selected in each
 fold.
 Maybe use frequencies?
\end_layout

\begin_layout Itemize
also the number of variables selected (metric of complexity)
\end_layout

\end_deeper
\begin_layout Paragraph*
OPTIONAL
\end_layout

\begin_layout Itemize
AIC/BIC? --> not the same model in each fold so maybe not possible
\end_layout

\begin_layout Itemize
Maybe some form of confusion matrices -- perhaps a grid of them with or
 without companion variance plots or other estimates of the range of results
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The area under the ROC (AUROC) curve assesses how accurately the predictions
 of the left-out observations are sorted.
 The methods are ranked at each site by AUROC and a mean rank (across sites)
 is computed for each method.
 The mean ranks are 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/wrbrooks/git/beauty_contest/figures/LOO-mean-ranks.eps

\end_inset


\begin_inset Graphics
	filename C:/Users/wrbrooks/git/beauty_contest/figures/LOYO-mean-ranks.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Mean ranks of the modeling techniques across the seven sites.
 At left, the ranks
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Performance over prediction from cross validation
\end_layout

\begin_layout Itemize
Maybe anecdotal showing fit over data set
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
In general, the GBM, and GBMCV, and AL techniques produced comparable results
 that were superior to the other techniques in terms of predictive performance.
 Since the GBMCV models take much longer to compute than the others, we
 will not include them in our more detailed analysis of the modeling results.
\end_layout

\begin_layout Standard
Which type of model is generally the best?
\end_layout

\begin_layout Standard
Under what conditions do some outperform others?
\end_layout

\begin_layout Standard
Relative value of overall best model versus methods that help trim variables?
 e.g.
 how valuable is it to reduce number of predictors? Further, which variables
 get cut? Expensive ones? Cheap ones?
\end_layout

\begin_layout Standard
How important is computational expense? Only an issue for model fitting
 --- not prediction, but worth quantifying.
 E.g.
 if GBM with cross validation takes hours, how much better? 
\end_layout

\begin_layout Standard
Model tuning for GBM versus GBM-CV --> notes on how GBM is faster with similar
 performance (e.g.
 CV is overkill maybe)
\end_layout

\begin_layout Section
Acknowledgments
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "C:/Users/wrbrooks/git/beauty_contest/references/beautycontest"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\end_body
\end_document
