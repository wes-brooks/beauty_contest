#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Beauty Contest
\end_layout

\begin_layout Abstract
Pithy, concise and informative.
 May bring the reader to tears due to the beauty of it.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The availability of large data sets for building regression models to predict
 the bacterial counts in beach water is both an opportunity and a challenge.
 
\end_layout

\begin_layout Subsection
Data Sources
\end_layout

\begin_layout Standard

\emph on
Possibly move this to the end of the section
\end_layout

\begin_layout Standard
Which sites
\end_layout

\begin_layout Standard
Where are they
\end_layout

\begin_layout Standard
What specific sources sources of data (plug EnDDAT)
\end_layout

\begin_layout Standard
Will include a map and tables
\end_layout

\begin_layout Subsection
Listing of specific statistical techniques
\end_layout

\begin_layout Subsubsection*
selection vs.
 modeling
\end_layout

\begin_layout Standard
Some methods are labeled 
\begin_inset Quotes eld
\end_inset

select
\begin_inset Quotes erd
\end_inset

, which means that they are used for variable selection only.
 Once the variables are selected, the regression model is estimated using
 ordinary least squares for the continuous regression methods, or ordinary
 logistic regression for the binary regression methods.
\end_layout

\begin_layout Standard
pls - this is just like the PLS method in Virtual Beach
\end_layout

\begin_layout Standard
gbm - just like in Virtual Beach
\end_layout

\begin_layout Standard
gbmcv - this one differs from gbm in that it chooses the number of trees
 by cross-validation, instead of the so-called "out-of-bag" estimator used
 by plain gbm.
 A little better but a lot slower than gbm.
\end_layout

\begin_layout Standard
- And these methods do variable selection in addition to model fitting:
\end_layout

\begin_layout Standard
adapt - uses the adaptive lasso for selection and fitting
\end_layout

\begin_layout Standard
adapt-select - adaptive lasso for selection, then take those variables and
 fit a model using R's lm() function 
\end_layout

\begin_layout Standard
adalasso-weighted - logistic regression via adaptive lasso with observations
 weighted based on their distance from the threshold
\end_layout

\begin_layout Standard
adalasso-weighted-select - like the above method but after selecting variables,
 they're fit using R's glm() function.
 
\end_layout

\begin_layout Standard
adalasso-unweighted - like adalasso-weighted but without the observation
 weights adalasso-unweighted-select - like the above method but after selecting
 variables, they're fit using R's glm() function.
 
\end_layout

\begin_layout Standard
galm - genetic algorithm used for selecting variables in a linear regression
 model 
\end_layout

\begin_layout Standard
galogistic-weighted - genetic algorithm used for selecting variables in
 a logistic regression model where observations are weighted based on their
 distance from the threshold galogistic-unweighted - just as above but without
 the observation weights.
 
\end_layout

\begin_layout Standard
spls - the sparse pls routine from the spls package 
\end_layout

\begin_layout Standard
spls-select - use sparse pls to select variables for linear regression,
 then fit the model using the lm() function.
\end_layout

\begin_layout Subsubsection
PLS
\end_layout

\begin_layout Standard
Partial least squares (PLS) regression is a tool for building regression
 models when there are many covariates available.
 PLS works by decomposing the covariates into mutually orthogonal components,
 with the components then used as the variables in a regression model.
 This is similar to the singular value decomposition (SVD), but the way
 PLS components are chosen ensures that they are aligned with the model
 output.
 On the other hand, SVD is sometimes criticised for decomposing the covariates
 into components that are unrelated to the model's output.
\end_layout

\begin_layout Standard
To use PLS, one must decide how many components to use in the model.
 The code for this paper follows the method described in (Brooks-Fienen-Corsi-20
12), using the PRESS statistic to select the number of components.
\end_layout

\begin_layout Subsubsection
GBM
\end_layout

\begin_layout Standard
The gradient boosting machine (GBM) of 
\begin_inset CommandInset citation
LatexCommand citet
key "Friedman-2001"

\end_inset

 has been called the 
\begin_inset Quotes eld
\end_inset

best off-the-shelf machine learning method
\begin_inset Quotes erd
\end_inset

 (Friedman-Hastie-Tibshirani-2001).
 A GBM model is a collection of many regression trees, and prediction is
 done by averaging the outputs of the several trees.
 The parameters shared by all GBM models in this study are:
\end_layout

\begin_layout Standard
Number of trees: 10000
\end_layout

\begin_layout Standard
Shrinkage parameter: 0.0005
\end_layout

\begin_layout Standard
Minimum observations per node: 5
\end_layout

\begin_layout Standard
Depth of each tree: 5
\end_layout

\begin_layout Standard
Bagging fraction: 0.5
\end_layout

\begin_layout Standard
One key decision when fitting a GBM model is how many regression trees to
 use.
 Our strategy is to produce a model using more trees than we think are necessary
, and then to prune back to the optimal number.
 The optimal number of trees is the number than minimizes some error statistic.
 GBM models are 
\end_layout

\begin_layout Paragraph
GBM
\end_layout

\begin_layout Standard
For these models, the 
\end_layout

\begin_layout Paragraph
GBM-CV
\end_layout

\begin_layout Subsubsection
SPLS
\end_layout

\begin_layout Standard
Sparse PLS (SPLS) is introduced in 
\begin_inset CommandInset citation
LatexCommand citet
key "Chun-Keles-2007"

\end_inset

.
 The SPLS method combines the orthogonal decompositions of PLS with lasso-type
 variable selection.
 
\end_layout

\begin_layout Paragraph
SPLS
\end_layout

\begin_layout Standard
This modeling method used SPLS to select relevant variables and fit the
 regression model.
\end_layout

\begin_layout Paragraph
SPLS-select
\end_layout

\begin_layout Standard
This modeling method used SPLS to select the relevant covariates and then
 fit an ordinary least squares regression model using those covariates.
\end_layout

\begin_layout Subsubsection
Adaptive Lasso
\end_layout

\begin_layout Standard
The adaptive lasso 
\begin_inset CommandInset citation
LatexCommand citet
key "Zou-2006"

\end_inset

 is a regression method that simultaneously selects relevant predictors
 and estimates their coefficients by adding an 
\begin_inset Formula $\ell_{1}$
\end_inset

 penalty to the negative log-likelihood.
 For linear regression, the adaptive lasso estiamtes 
\begin_inset Formula $\bm{{\beta}}$
\end_inset

 to minimize the criterion 
\begin_inset Formula $\sum_{i=1}^{n}(y_{i}-X_{i}\beta)^{2}+\lambda\sum_{j=1}^{p}\frac{{|\beta_{j}|}}{\tilde{{|\beta_{j}|^{\gamma}}}}$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is a tuning parameter and 
\begin_inset Formula $\tilde{{\bm{{\beta}}}}$
\end_inset

 is a consistent estimate of the regression coefficients.
\end_layout

\begin_layout Standard
In this work, 
\begin_inset Formula $\gamma$
\end_inset

 is set to one and the adaptive lasso tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 is selected to minimize the bias-corrected AIC of 
\begin_inset CommandInset citation
LatexCommand citet
key "Hurvich-Simonoff-Tsai-1998"

\end_inset

.
\end_layout

\begin_layout Paragraph
Gaussian
\end_layout

\begin_layout Subparagraph
adapt
\end_layout

\begin_layout Standard
Uses the adaptive lasso of 
\begin_inset CommandInset citation
LatexCommand citet
key "Zou-2006"

\end_inset

 to select variables and estimate their coefficients.
 
\end_layout

\begin_layout Subparagraph
adapt-select
\end_layout

\begin_layout Standard
Uses the adaptive lasso of 
\begin_inset CommandInset citation
LatexCommand citet
key "Zou-2006"

\end_inset

 to select variables, with the corrected AIC for selection of the tuning
 parameter.
 The results of the variable selection are used in a multiple linear regression
 model.
\end_layout

\begin_layout Paragraph
Logistic
\end_layout

\begin_layout Standard
Logistic regression (McCullagh-Nelder-1989) is a method for fitting a regression
 model to binary data.
 For our models, bacterial concentration was replaced by an indicator of
 whether the observed concentration exceeded the regulatory standard.
 
\end_layout

\begin_layout Subparagraph
weighted
\end_layout

\begin_layout Subparagraph
weighted-select
\end_layout

\begin_layout Subparagraph
Unweighted
\end_layout

\begin_layout Subparagraph
Unweighted-select
\end_layout

\begin_layout Subsubsection
Genetic algorithm
\end_layout

\begin_layout Standard
The genetic algorithm 
\begin_inset CommandInset citation
LatexCommand citep
key "Fogel-1998"

\end_inset

 is a variable-selection method that works by analogy to natural selection,
 where so-called chromosomes represent regression models.
 A variable is included in the model if the corresponding element of the
 chrmosome is one, but not otherwise.
 Chromosomes are produced in successive generations, where the first generation
 is produced randomly and subsequent generations are produced by combining
 chromosomes from the current generation, with additional random drift.
 The chance that a chromosome in the current generation will produce offspring
 in the next generation is a determined by its fitness.
 The fitness of each chromosome is calculated by the corrected Akaike Informatio
n Criterion (AICc) 
\begin_inset CommandInset citation
LatexCommand citet
key "Akaike-1973,Hurvich-Tsai-1989"

\end_inset

.
\end_layout

\begin_layout Standard
Our implementation used 100 generations and each generation contained 200
 chromosomes.
 
\end_layout

\begin_layout Paragraph*
linear
\end_layout

\begin_layout Standard
The genetic algorithm for linear regression is 
\end_layout

\begin_layout Paragraph
logistic
\end_layout

\begin_layout Standard
Two options were considered for observation weights for building logistic
 regression models via the genetica algorithm.
 
\end_layout

\begin_layout Subparagraph
ga-unweighted
\end_layout

\begin_layout Subparagraph
ga-weighted
\end_layout

\begin_layout Subsection
Implementation for beach regression
\end_layout

\begin_layout Standard
Include a table with pre/post processing discussion
\end_layout

\begin_layout Standard
This includes tuning of parameters
\end_layout

\begin_layout Standard
Some specific data issues because we are estimating a threshold exceedence
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
Modeling methods were assessed by their performance in cross-validation.
 Two separate assesments were made: leave-one-out and leave-one-year-out
 
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
How did we evaluate the performance of each technique on all the different
 data sets
\end_layout

\begin_layout Itemize
for all cases ---> AUC (ROC curve)
\end_layout

\begin_layout Itemize
continuous variables using PRESS (skill --> Like Nash-Sutcliffe/R^2 over
 the fitted data) 
\end_layout

\begin_layout Itemize
True/False Positives/Negatives (needs a threshold)
\end_layout

\begin_layout Itemize
Which variables are selected for models where variable reduction takes place
\end_layout

\begin_deeper
\begin_layout Itemize
challenge regarding the fact that different variables are selected in each
 fold.
 Maybe use frequencies?
\end_layout

\begin_layout Itemize
also the number of variables selected (metric of complexity)
\end_layout

\end_deeper
\begin_layout Paragraph*
OPTIONAL
\end_layout

\begin_layout Itemize
AIC/BIC? --> not the same model in each fold so maybe not possible
\end_layout

\begin_layout Itemize
Maybe some form of confusion matrices -- perhaps a grid of them with or
 without companion variance plots or other estimates of the range of results
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Itemize
Performance over prediction from cross validation
\end_layout

\begin_layout Itemize
Maybe anecdotal showing fit over data set
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
The GALM, GBM, and GBMCV models were generally superior to the others.
 These models were nearly equal to each other in terms of predictive performance.
 Of these, the GBMCV models take much longer to computer than the others,
 so we will focus our discussion of the results on the GBM and GALM models.
\end_layout

\begin_layout Standard
Which type of model is generally the best?
\end_layout

\begin_layout Standard
Under what conditions do some outperform others?
\end_layout

\begin_layout Standard
Relative value of overall best model versus methods that help trim variables?
 e.g.
 how valuable is it to reduce number of predictors? Further, which variables
 get cut? Expensive ones? Cheap ones?
\end_layout

\begin_layout Standard
How important is computational expense? Only an issue for model fitting
 --- not prediction, but worth quantifying.
 E.g.
 if GBM with cross validation takes hours, how much better? 
\end_layout

\begin_layout Standard
Model tuning for GBM versus GBM-CV --> notes on how GBM is faster with similar
 performance (e.g.
 CV is overkill maybe)
\end_layout

\begin_layout Section
Acknowledgments
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/wesley/git/beauty_contest/references/beautycontest"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
