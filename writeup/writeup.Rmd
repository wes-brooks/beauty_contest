---
title: "Comparing methods for predicting health advisories for beach water"
author: "Wesley Brooks, Rebecca Carvin, Steve Corsi"
output: pdf_document
bibliography: ../references/beautycontest.bib
---

# Abstract
Pithy, concise and informative. May bring the reader to tears due to the beauty of it.

# Introduction

With input from the US Environmental Protection Agency, the state of Wisconsin has established regulatory standards for beach water quality, which states that a warning is to be posted when the concentration of E. coli exceeds $235$ CFU / $100$ mL. (Is that statement correct?) The goal of modeling the bacterial concentration is to predict in advance when the concentration will exceed the limit.

# Methods

The availability of large data sets for building regression models to predict the bacterial counts in beach water is both an opportunity and a challenge. 

## Data Sources

Possibly move this to the end of the section

Which sites

Where are they

What specific sources sources of data (plug EnDDAT)

Will include a map and tables

## Definitions

At any site, denote the predictor variables by $X$, which is an $n\times p$ matrix where $n$ is the number of observations and $p$ is the number of predictors. The vector of $n$ observations of bacterial concentration is denoted $y$. The mathematical model relating $y$ to $X$ is the function $\mu(X, y)$. Denote the regulatory standard by $\delta$ and the decision threshold by $\hat{{\delta}}$.

## Listing of specific statistical techniques

Fourteen different regression modeling techniques were considered. Each technique uses one of five modeling algorithms: GBM, the adaptive lasso, the genetic algorithm, PLS, or sparse PLS. Each technique is applied to either continuous or binary regression and to either variable selection and model estimation, or variable selection only.

#### Continuous vs. binary regression

The goal of predicting exceednaces of the water quality standard is approached in two ways: one is to predict the bacterial concentration and then compare the prediction to a threshold, which is referred to as continuous modeling. The other is referred to as binary modeling, in which we predict the state of the binary indicator $z_{i}$:

$$ z_{i}=I(y_{i}>\delta) $$


The indicator is coded as zero when the concetration is below the regulatory standard and one when the concentration exceeds the standard. All of the binary modeling techniques herein use logistic regression, which uses the logistic link function g
to translate $p_{i}=E(z_{i})$ - the probability that the $i$th observation is an exceedance - into an unbounded quantity.

$$ g(p_{i})=\log\frac{p_{i}}{1-p_{i}} $$


#### Weighting of observations in binary regression

A weighting scheme was implemented for some of the binary regression techniques. In the weighting scheme, observations were given weights $w_{i}$ where:

$$ w_{i}=    (y_{i}-\delta)/\hat{{sd}}(y)
\hat{{sd}}(y)=	\sqrt{\sum_{i=1}^{n}(y_{i}-\bar{{y}})^{2}/n}
\bar{{y}}=	\sum_{i=1}^{n}y_{i}/n $$


That is, the weights are equal to the number of standard deviations that the observed concentration lies from the regulatory threshold $\delta$. Any technique that was implemented with this weighting scheme was separately implemented without any weighting of the observations. The techniques are thus labeled weighted and unweighted, respectively.

#### Modeling or selection only

The regression techniques adaptive lasso and sparse PLS include a variable selection step. Some methods are labeled “select”, which means that they are used for variable selection only. In these cases, once the predictor variables are selected, the regression model using those predictors is estimated using ordinary least squares for the continuous regression techniques, or ordinary logistic regression for the binary regression techniques.

### GBM

GBM refers to the gradient boosting machine (GBM) [@Friedman-2001]. A GBM model is a so-called random forest model - a collection of many regression trees. Prediction is done by averaging the outputs of the trees. Two GBM-based techniques are explored - we refer to them as GBM-OOB and GBM-CV. The difference is in how the optimal number of trees is determined - GBM-CV selects the number of trees in a model using leave-one-out CV, while GBM-OOB uses the so-called out-of-bag error estimate. The CV method is much slower (it has to construct as many random forests as there are observations, while the OOB method only requires computing a single random forest) but GBMCV should more accurately estimate the prediction error. All the GBM-OOB and GBM-CV models share the following settings:

Number of trees: 10000

Shrinkage parameter: 0.0005

Minimum observations per node: 5

Depth of each tree: 5

Bagging fraction: 0.5

### Adaptive Lasso

The adaptive lasso is a regression method that simultaneously selects relevant predictors and estimates their coefficients by adding a penalty to the sum of the squared residuals [@Zou-2006]. For continuous modeling techniques the adaptive lasso selects the predictors for linear regression, estimating $\hat{\bm{\beta}}$ minimize the criterion 

$$ \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p \frac{|\beta_{j}|}{|\tilde{\beta}_{j}|^{\gamma}}, $$

where $\lambda$ is a tuning parameter and $\tilde{\bm{\beta}}$ is a consistent estimate of the regression coefficients. For binary modeling, the adaptive lasso maximizes the penalized log-likelihood

$$ \sum_{i=1}^n \left[-(1 - y_i) X_i \beta - \log \left\{ 1 + \exp\left(-X_i \beta \right) \right\} \right] + \lambda \sum_{j=1}^p \frac{|\beta_j|}{|\tilde{\beta}_j|^{\gamma}}, $$

where, as was the case for continuous response, $\tilde{\bm{\beta}}$ is a consistent estimate of the regression coefficients, which is calculated by ordinary logistic regression.

In this work, $\gamma=1$, $\tilde{\bm{\beta}}$ are estimated individually by a univariate linear or logistic regression (it is necessary to estimate the coefficients individually because there are usually more covariates than observations), and the adaptive lasso tuning parameter $\lambda$ is selected to minimize the AICc [@Hurvich-Simonoff-Tsai-1998].

Five of the modeling techniques were based on the adaptive lasso - one for continuous response (AL), and four for binary response (AL-logistic-weighted, AL-logistic-unweighted, AL-logistic-weighted-select, AL-logistic-unweighted-select). The four binary response techniques are the combination of weighted versus unweighted, and selection-only versus selection-and-estimation.

### Genetic algorithm

The genetic algorithm is a variable-selection method that works by analogy to natural selection, where so-called chromosomes represent regression models [@Fogel-1998]. A variable is included in the model if the corresponding element of the chromosome is one, but not otherwise. Chromosomes are produced in successive generations, where the first generation is produced randomly and subsequent generations are produced by combining chromosomes from the current generation, with additional random drift. The chance that a chromosome in the current generation will produce offspring in the next generation is an increasing function of its fitness. The fitness of each chromosome is calculated by the corrected Akaike Information Criterion (AICc) [@Akaike-1973; @Hurvich-Tsai-1989].

The implementations in this study used 100 generations, with each generation consisting of 200 chromosomes. The genetic algorithm method (GA) is the default for linear regression modeling in Virtual Beach [@Cyterski-Brooks-Galvin-Wolfe-Carvin-Roddick-Fienen-Corsi-2013]. This study also investigates two genetic algorithm methods for logistic regression: one weighted (GA-logistic-weighted) and one unweighted (GA-logistic-unweighted).

### PLS

Partial least squares (PLS) regression is a tool for building regression models with many covariates [@Wold-Sjostrum-Eriksson-2001]. PLS works by decomposing the covariates into mutually orthogonal components, with the components then used as the variables in a regression model. This is similar to principal components regression (PCR), but the way PLS components are chosen ensures that they are aligned with the model output. On the other hand, PCR is sometimes criticised for decomposing the covariates into components that are unrelated to the model's output. To use PLS, one must decide how many components to use in the model. This study follows the method described in [@Brooks-Fienen-Corsi-2013], using the PRESS statistic to select the number of components.

### SPLS

Sparse PLS (SPLS) combines the orthogonal decompositions of PLS with the sparsity of lasso-type variable selection [@Chun-Keles-2007]. To do so, SPLS uses two tuning parameters: one that controls the number of orthogonal components and one that controls the lasso-type penalty. The optimal parameters are those that minimize the mean squared prediction error (MSEP) over a two-dimensional grid search. The MSEP is estimated by 10-fold cross-validation. SPLS was used for both selection-and-estimation (SPLS) and selection-only (SPLS-select).

## Implementation for beach regression

The response variable for our continuous regression models is the base-10 logarithm of the E. coli concentration. For the binary regression models, the response variable is an indicator of whether the concentration exceeds the regulatory threshold $\delta=235$ CFU/mL. Transformations were applied to some of the data during pre-processing: the beach water turbidity and the discharge of tributaries near each beach were log-transformed, and rainfall variables were all square root transformed.

Include a table with pre/post processing discussion

```{r load-packages, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
    library(dplyr)
    library(reshape2)
    library(ggplot2)
	library(brooks)
	library(xtable)
```

```{r compile-results, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
	#Load the raw results of the beauty contest:
	setwd("..")
	load("beauty_contest.RData")

	source("R/bootstrap-summarize.r")
	source("R/bootstrap-summarize-annual.r")
    source("scratch/QFNTD.r")
	source("R/get_annual_predictive_performance.r")
	source("R/pair-tables.r")
```

## Cross Validation

Our assessment of the modeling techniques is based on their performance in predicting exceedances of the regulatory standard. Two types of cross validation was used to measure the performance in prediction: leave-one-out (LOO) and leave-one-year-out (LOYO). In LOO CV, one observation is held out for validation while the rest of the data is used to train a model. The model is used to predict the concentration of that held out observation, and the process is repeated for each observation. Each cycle of LOYO CV holds out an entire year's worth of data for validation instead of a single observation. It is intended to approximate the performance of the modeling technique under a typical use case: a new model is estimated before the start of each annual beach season and then used for predicting exceedances during the season. The LOYO models in this study were estimated using all the available data except for the held out year, even that from future years. So for instance the 2012 models were estimated using the 2010-2011 and 2013 data.

Some methods also used cross-validation internally to select tuning parameters. In those cases the internal CV was done using only the model data, and never looking at the held-out observation(s). This process is separate from - and does not affect - the CV to assess predictive performance.


## Comparing methods, and quantifying uncertainty in the ranks

Results were compiled into one table for each site, such as the one below which cotains the results of running the contest at Hika. Each observation corresponds to a row in the table. The results table has a column for the observed log *E. coli* concentration and, for each method, columns for the predicted concentration by LOO CV and by LOYO CV. From the table, we can calculate the predictive error sum of squares (PRESS) and the area under the receiver operating characteristic (ROC) curve (AUROC), which are the statistics we use to summarize performance of the modeling methods.

------------------------------------------------------------
                 `pls`   `pls`            `adapt`   `adapt`      
 Row   Actual    (LOO)   (LOYO)   \dots   (LOO)     (LOYO) 
----- --------  ------- -------- ------- --------- --------- 
1      2.54      2.35     2.22   \dots   2.29      2.55 

2      2.59      1.87     1.79   \dots   1.91      1.23

\vdots \vdots    \vdots   \vdots \dots   \vdots    \vdots

166    1.57      1.93     2.06   \dots   1.83      2.07 

167    3.38      1.84     2.01   \dots   1.80      1.71
-----  ------    ------   ------ -----   ------    ------ 

In order to identify which modeling methods have the best performance across all sites, the summary statistics at each site were converted to ranks. We report the mean rank of each method across the sites. Uncertainty in the rankings is quantified by the bootstrap. Since PRESS and AUROC are functions of the results tables, the bootstrap procedure is carried out by resampling the rows of each results table and recalculating the ranks for each bootstrap sample. We used $1001$ bootstrap samples of each results table in the analysis that follows.

# Results

## AUROC

The ROC curve is an assessment of how well predictions are sorted. The AUROC averages the model's performance over the range of possible thresholds. A model which perfectly separates exceedances from non-exceedances in prediction has an AUROC of one, while a model that predicts exceedances no better than a coin flip has an AUROC of $0.5$.

The mean LOO and LOYO ranks for all the methods are plotted in Figure [fig:auroc-boxplot]. The three top-ranked methods were GBM-CV, GBM-OOB, and the adaptive lasso. In order to facilitate a pairwise comparison between modeling methods, Tables [table:auroc.pairs.annual] (for the leave-one-year-out analysis) and [table:auroc.pairs] (for the leave-one-out analysis) show the frequency that the mean AUROC rank of GBM-OOB, GBM-CV, or the adaptive lasso exceeded each of the other modeling methods. 

```{r auroc-barchart, fig.width=14, fig.cap="Mean ranking of the methods by area under the ROC curce (AUROC) across all sites (higher is better). The error bars are 90% confidence intervals computed by the bootstrap. At left are the AUROC rankings from the leave-one-year-out cross validation, at right are the AUROC rankings from the leave-one-out cross validation.", fig.subcap=c('LOO', 'LOYO'), echo=FALSE, warning=FALSE, message=FALSE}
    multiplot(roc.barchart.annual, roc.barchart, cols=2)
```

```{r auroc-tables, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
    xtable(auroc.naive.pairs.annual[1:3,], caption="Under leave-one-year-out cross validation, how often the mean AUROC rank of GBM-OOB, GBM-CV, or the adaptive lasso (in the rows) exceeded that of the other methods (in the columns).", label="table:auroc.pairs.annual") %>% print(sanitize.colnames.function=tabulate_headers, rotate.colnames=TRUE)
    xtable(auroc.naive.pairs[1:3,], caption="Under leave-one-out cross validation, how often the mean AUROC rank of GBM-OOB, GBM-CV, or the adaptive lasso (in the rows) exceeded that of the other methods (in the columns).", label="table:auroc.pairs") %>% print(sanitize.colnames.function=tabulate_headers, rotate.colnames=TRUE)
```

## PRESS

While AUROC quantifies how well a model sorts exceedances and non-exceedances, PRESS measures how accurrately a model's predictions match the observed bacterial concentration. The PRESS can only be computed for continuous regression methods. Let the model's predictions be denoted $\tilde{y}_i$ and letting the actual observed bacterial concentrations be denoted $y_{i}$ for $i=1,\dots,n$ where $n$ is the total number of predictions. Then PRESS computed as follows:
    
$$    \text{PRESS}=\sum_{i=1}^{n}\left(\tilde{y}_{i}-y_{i}\right)^{2}.$$


The PRESS statistic is of interest because a good model should accurately predict the bacterial concentration, but AUROC is a more important as a metric of model performance than the PRESS because it directly measures the ability of a model to separate exceedances from non-exceedances. That said, we expect the two statistics to usually agree on which modeling methods are the best.

The rankings of the methods by PRESS are plotted in Figure [fig:press-boxplot]. The top three techniques under both LOO and LOYO analysis were GBM-CV, GBM-OOB, and the adaptive lasso. The pairwise comparison of modeling methods by PRESS are in Tables [table:auroc.pairs.annual] (for the leave-one-year-out analysis) and [table:auroc.pairs] (for the leave-one-out analysis).

```{r press-barchart, fig.width=14, fig.cap="Mean ranking of the methods by predictive error sum of squares (PRESS) across all sites (higher is better). The error bars are 90% confidence intervals computed by the bootstrap. At left are the PRESS rankings from the leave-one-year-out cross validation, at right are the PRESS rankings from the leave-one-out cross validation.", fig.subcap=c('LOO', 'LOYO'), echo=FALSE, warning=FALSE, message=FALSE}
    multiplot(press.barchart.annual, press.barchart, cols=2)
```
    
    
```{r press-tables, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
    xtable(press.pairs.annual[1:3,], caption="Under leave-one-year-out cross validation, how often the mean PRESS rank of GBM-OOB, GBM-CV, or the adaptive lasso (in the rows) exceeded that of the other methods (in the columns).", label="table:press.pairs.annual") %>% print(sanitize.colnames.function=tabulate_headers, rotate.colnames=TRUE)
	xtable(press.pairs[1:3,], caption="Under leave-one-out cross validation, how often the mean PRESS rank of GBM-OOB, GBM-CV, or the adaptive lasso (in the rows) exceeded that of the other methods (in the columns).", label="table:press.pairs") %>% print(sanitize.colnames.function=tabulate_headers, rotate.colnames=TRUE)
```
    
## Classification of responses

The real-world performance of any model for predicting exceedances will be measured by how well it distinguishes between exceedances and nonexceedances. This is quantified similar to AUROC except that AUROC is an average over all possible thresholds instead of an assessment of performance for a specific choice of threshold. Combined with a realistic carefully selected decision threshold, the LOYO CV results were used to simulate real-world use of the models.

Intuitively, the decision threshold should adapt to the conditions that are observed in the beach's training data. If, for instance, exceedances are rare in the training data, then we expect few exceedances in the future, and should set the threshold high to reflect this prior expectation. On the other hand, if the bacterial concentration often exceeds the regulatory standard, then the decision threshold should be set lower in order to properly flag more of those exceedances. This intuition was encoded into how the decision threshold was set for the LOYO models. Specifically, the decision threshold was set to the $q$
th quantile of the fitted values of non-exceedances in the training set, where $1-q$
is the proportion of exceedances in the training set.

By both AUROC and PRESS, and for both LOO and LOYO analyses, the three highest-ranked modeling methods were  GBM-CV, GBM-OOB, and the adaptive lasso. Here we assess the performance of GBM-OOB and the adaptive lasso because the GBM-OOB and GBM-CV methods are very similar and fitting a GBM-CV takes many times longer than a GBM-OOB model.

In Figure [fig:counts-barcharts], we look at the counts on a per-beach basis of four categories of decisions: true positives (correctly posting an advisory), true negatives (correctly not posting an advisory), false positives (wrongly posting an advisory) and false negatives (wrongly not posting an advisory).  In most cases, the counts are similar between the two techniques, with GBM-OOB tending to make a few more correct decisions. There are exceptions where adaptive lasso makes more correct decisions (e.g., Hika and Red Arrow).

```{r counts-barcharts, fig.width=16, fig.height=16, fig.cap='Mean ranks of the modeling techniques across the seven sites. At left are the mean ranks under leave-one-out cross validation, at the right are the mean ranks from leave-one-year-out cross validation.', fig.subcap=c('LOO', 'LOYO'), echo=FALSE, warning=FALSE, message=FALSE}
    multiplot(plotlist=pp, cols=3)
```

## Variable selection

It was noted in Section [Classification of responses] that GBM-OOB and the adaptive lasso are the two of the three best-ranked methods. One difference between the two is that the adaptive lasso does variable selection while GBM-OOB uses all of the available covariates. We look here at how many variables were used in the adaptive lasso models compared to the GBM-OOB models.

The variable counts are displayed in Figure [fig:varselect-barchart]. At most of the sites, the adaptive lasso uses only a small fraction of the available covariates, but at Point the adaptive lasso uses almost all of the available covariates. That's because the variable selection criterion we used (AICc) is intended to minimize prediction error. As the amount of data increases, we accumulate enough information to begin to discern the effect even of covariates that are only slightly correlated with the response. As our dataset grows, then, we should expect more covariates to be selected for an adaptive lasso model, and Point has far more observations than the other sites.

```{r varselect-barchart, fig.cap="At each site, the number of variables that were selected for the adaptive lasso and GBM-OOB models, divided by whether they were collected from automated web services, or measured manually at the beach.", echo=FALSE, warning=FALSE, message=FALSE}
    print(nvar.plot)
```
    
# Discussion

Which type of model is generally the best?

Under what conditions do some outperform others?

Relative value of overall best model versus methods that help trim variables? e.g. how valuable is it to reduce number of predictors? Further, which variables get cut? Expensive ones? Cheap ones?

How important is computational expense? Only an issue for model fitting --- not prediction, but worth quantifying. E.g. if GBM with cross validation takes hours, how much better? 

Model tuning for GBM-OOB versus GBM-CV --> notes on how GBM-OOB is faster with similar performance (e.g. CV is overkill maybe)

# Acknowledgments

# References

