#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\usepackage{subfig}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Beauty Contest
\end_layout

\begin_layout Author
Wesley Brooks, Steve Corsi, Rebecca Carvin
\end_layout

\begin_layout Abstract
Pithy, concise and informative.
 May bring the reader to tears due to the beauty of it.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The availability of large data sets for building regression models to predict
 the bacterial counts in beach water is both an opportunity and a challenge.
 
\end_layout

\begin_layout Subsection
Data Sources
\end_layout

\begin_layout Standard

\emph on
Possibly move this to the end of the section
\end_layout

\begin_layout Standard
Which sites
\end_layout

\begin_layout Standard
Where are they
\end_layout

\begin_layout Standard
What specific sources sources of data (plug EnDDAT)
\end_layout

\begin_layout Standard
Will include a map and tables
\end_layout

\begin_layout Subsection
Definitions
\end_layout

\begin_layout Standard
At any site, denote the predictor variables by 
\begin_inset Formula $X$
\end_inset

, which is an 
\begin_inset Formula $n\times p$
\end_inset

 matrix where 
\begin_inset Formula $n$
\end_inset

 is the number of observations and 
\begin_inset Formula $p$
\end_inset

 is the number of predictors.
 The vector of 
\begin_inset Formula $n$
\end_inset

 observations of bacterial concentration is denoted 
\begin_inset Formula $y$
\end_inset

.
 The mathematical model relating 
\begin_inset Formula $y$
\end_inset

 to 
\begin_inset Formula $X$
\end_inset

 is the function 
\begin_inset Formula $\mu(X,$
\end_inset

y).
\end_layout

\begin_layout Subsection
Goals
\end_layout

\begin_layout Standard
With input from the US Environmental Protection Agency, the state of Wisconsin
 has established regulatory standards for beach water quality, which states
 that a warning is to be posted when the concentration of E.
 coli exceeds 235 CFU / 100 mL.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is that statement correct?
\end_layout

\end_inset

 The goal of modeling the bacterial concentration is to predict in advance
 when the concentration will exceed the limit.
 For the discussion to come, denote the regulatory standard by 
\begin_inset Formula $\delta$
\end_inset

.
 Each model has two essential components: the mathematical model itself,
 
\begin_inset Formula $\mu(X,y)$
\end_inset

, and a decision threshold, 
\begin_inset Formula $\hat{{\delta}}$
\end_inset

, that is used to interpret the model's predictions.
\end_layout

\begin_layout Standard
There is no reason that 
\begin_inset Formula $\delta$
\end_inset

 and 
\begin_inset Formula $\hat{{\delta}}$
\end_inset

 must be equal.
 Rather, each 
\begin_inset Formula $\hat{{\delta}}$
\end_inset

 should be chosen so that the number of false positives and false negatives
 are balanced to the satisfaction of the beach manager.
 Using cross validation allows us to set 
\begin_inset Formula $\hat{{\delta}}$
\end_inset

 to expect that the performance over future data will resemble what was
 observed over the testing data.
\end_layout

\begin_layout Subsection
Listing of specific statistical techniques
\end_layout

\begin_layout Standard
Fourteen different regression modeling techniques were considered.
 Each technique uses one of five modeling algorithms: GBM, the adaptive
 lasso, the genetic algorithm, PLS, or sparse PLS.
 Each technique is aplied to either continuous or binary regression and
 to either modeling, or variable selection only.
\end_layout

\begin_layout Subsubsection*
Continuous or binary regression
\end_layout

\begin_layout Standard
The goal of predicting exceednaces of the water quality standard is approached
 in two ways: one is to predict the bacterial concentration and then compare
 the prediction to a threshold, which is referred to as continuous modeling.
 The other is referred to as binary modeling, in which we predict the state
 of the binary indicator 
\begin_inset Formula $z_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
z_{i}=I(y_{i}>\delta)
\]

\end_inset


\end_layout

\begin_layout Standard
The indicator is coded as zero when the concetration is below the regulatory
 standard and one when the concentration exceeds the standard.
 All of the binary modeling techniques herein use logistic regression, which
 uses the logistic link function 
\begin_inset Formula $g$
\end_inset

 to translate 
\begin_inset Formula $p_{i}=E(z_{i})$
\end_inset

 - the probability that the 
\begin_inset Formula $i$
\end_inset

th observation is an exceedance - into an unbounded quantity.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(p_{i})=\log\frac{p_{i}}{1-p_{i}}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Weighting of observations in binary regression
\end_layout

\begin_layout Standard
A weighting scheme was implemented for some of the binary regression techniques.
 In the weighting scheme, observations were given weights 
\begin_inset Formula $w_{i}$
\end_inset

 where:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
w_{i}= & (y_{i}-\delta)/\hat{{sd}}(y)\\
\hat{{sd}}(y)= & \sqrt{\sum_{i=1}^{n}(y_{i}-\bar{{y}})^{2}/n}\\
\bar{{y}}= & \sum_{i=1}^{n}y_{i}/n
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
That is, the weights are equal to the number of standard deviations that
 the observed concentration lies from the regulatory threshold 
\begin_inset Formula $\delta$
\end_inset

.
 Any technique that was implemented with this weighting scheme was separately
 implemented without any weighting of the observations.
 The techniques are thus labeled weighted and unweighted, respectively.
\end_layout

\begin_layout Subsubsection*
Modeling or selection only
\end_layout

\begin_layout Standard
The regression techniques adaptive lasso and sparse PLS include a variable
 selection step.
 , Some methods are labeled 
\begin_inset Quotes eld
\end_inset

select
\begin_inset Quotes erd
\end_inset

, which means that they are used for variable selection only.
 In these cases, once the predictor variables are selected, the regression
 model using those predictors is estimated using ordinary least squares
 for the continuous regression techniques, or ordinary logistic regression
 for the binary regression techniques.
\end_layout

\begin_layout Subsubsection
GBM
\end_layout

\begin_layout Standard
GBM refers to the gradient boosting machine (GBM) of 
\begin_inset CommandInset citation
LatexCommand citet
key "Friedman-2001"

\end_inset

.
 A GBM model is a so-called random forest model - a collection of many regressio
n trees.
 Prediction is done by averaging the outputs of the trees.
 Two GBM-based techniques are explored - we refer to them as GBM and GBMCV.
 The difference is in how the optimal number of trees is determined - GBMCV
 selects the number of trees in a model using leave-one-out CV, while GBM
 uses the so-called out-of-bag (OOB) error estimate.
 The CV method is much slower (it has to construct as many random forests
 as there are observations, while the OOB method only requires computing
 a single random forest) but GBMCV should more accurately estimate the predictio
n error.
 All the GBM and GBMCV models share the following settings:
\end_layout

\begin_layout Standard
Number of trees: 10000
\end_layout

\begin_layout Standard
Shrinkage parameter: 0.0005
\end_layout

\begin_layout Standard
Minimum observations per node: 5
\end_layout

\begin_layout Standard
Depth of each tree: 5
\end_layout

\begin_layout Standard
Bagging fraction: 0.5
\end_layout

\begin_layout Subsubsection
Adaptive Lasso
\end_layout

\begin_layout Standard
The adaptive lasso 
\begin_inset CommandInset citation
LatexCommand citep
key "Zou-2006"

\end_inset

 is a regression method that simultaneously selects relevant predictors
 and estimates their coefficients by adding a penalty to the sum of the
 squared residuals.
 For continuous modeling techniques the adaptive lasso selects the predictors
 for linear regression, estimating 
\begin_inset Formula $\hat{{\bm{\beta}}}$
\end_inset

 minimize the criterion 
\begin_inset Formula $\sum_{i=1}^{n}(y_{i}-X_{i}\beta)^{2}+\lambda\sum_{j=1}^{p}\frac{{|\beta_{j}|}}{\tilde{{|\beta_{j}|^{\gamma}}}}$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is a tuning parameter and 
\begin_inset Formula $\tilde{{\bm{\beta}}}$
\end_inset

 is a consistent estimate of the regression coefficients.
\end_layout

\begin_layout Standard
For binary modeling, the adaptive lasso maximizes the penalized log-likelihood
 
\begin_inset Formula $\sum_{i=1}^{n}\left[-\left(1-y_{i}\right)X_{i}\beta-\log\left\{ 1+\exp\left(-X_{i}\beta\right)\right\} \right]+\lambda\sum_{j=1}^{p}\frac{{|\beta_{j}|}}{\tilde{{|\beta_{j}|^{\gamma}}}}$
\end_inset

, where, as was the case for continuous response, 
\begin_inset Formula $\tilde{\bm{\beta}}$
\end_inset

 is a consistent estimate of the regression coefficients, which is calculated
 by ordinary logistic regression.
\end_layout

\begin_layout Standard
In this work, 
\begin_inset Formula $\gamma=1$
\end_inset

 , 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tilde{{\bm{\beta}}}$
\end_inset

 are estimated individually by a univariate linear or logistic regression
 (it is necessary to estimate the coefficients individually because there
 are usually more covariates than observations),
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 and the adaptive lasso tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 is selected to minimize the AICc 
\begin_inset CommandInset citation
LatexCommand citep
key "Hurvich-Simonoff-Tsai-1998"

\end_inset

.
\end_layout

\begin_layout Standard
Five of the modeling techniques were based on the adaptive lasso - one for
 continuous response (AL), and four for binary response (AL-logistic-weighted,
 AL-logistic-unweighted, AL-logistic-weighted-select, AL-logistic-unweighted-sel
ect).
 The four binary response techniques are the combination of weighted versus
 unweighted, and selection-only versus selection-and-estimation.
\end_layout

\begin_layout Subsubsection
Genetic algorithm
\end_layout

\begin_layout Standard
The genetic algorithm 
\begin_inset CommandInset citation
LatexCommand citep
key "Fogel-1998"

\end_inset

 is a variable-selection method that works by analogy to natural selection,
 where so-called chromosomes represent regression models.
 A variable is included in the model if the corresponding element of the
 chromosome is one, but not otherwise.
 Chromosomes are produced in successive generations, where the first generation
 is produced randomly and subsequent generations are produced by combining
 chromosomes from the current generation, with additional random drift.
 The chance that a chromosome in the current generation will produce offspring
 in the next generation is an increasing function of its fitness.
 The fitness of each chromosome is calculated by the corrected Akaike Informatio
n Criterion (AICc) 
\begin_inset CommandInset citation
LatexCommand citet
key "Akaike-1973,Hurvich-Tsai-1989"

\end_inset

.
\end_layout

\begin_layout Standard
The implementations in this study used 100 generations, with each generation
 consisting of 200 chromosomes.
 The genetic algorithm method (GA) is the default for linear regression
 modeling in Virtual Beach 
\begin_inset CommandInset citation
LatexCommand citep
key "Cyterski-Brooks-Galvin-Wolfe-Carvin-Roddick-Fienen-Corsi-2013"

\end_inset

.
 This study also investigates two genetic algorithm methods for logistic
 regression: one weighted (GA-logistic-weighted) and one unweighted (GA-logistic
-unweighted).
\end_layout

\begin_layout Subsubsection
PLS
\end_layout

\begin_layout Standard
Partial least squares (PLS) regression is a tool for building regression
 models with many covariates 
\begin_inset CommandInset citation
LatexCommand citep
key "Wold-Sjostrum-Eriksson-2001"

\end_inset

.
 PLS works by decomposing the covariates into mutually orthogonal components,
 with the components then used as the variables in a regression model.
 This is similar to principal components regression (PCR), but the way PLS
 components are chosen ensures that they are aligned with the model output.
 On the other hand, PCR is sometimes criticised for decomposing the covariates
 into components that are unrelated to the model's output.
\end_layout

\begin_layout Standard
To use PLS, one must decide how many components to use in the model.
 This study follows the method described in 
\begin_inset CommandInset citation
LatexCommand citet
key "Brooks-Fienen-Corsi-2013"

\end_inset

, using the PRESS statistic to select the number of components.
\end_layout

\begin_layout Subsubsection
SPLS
\end_layout

\begin_layout Standard
Sparse PLS (SPLS) is introduced in 
\begin_inset CommandInset citation
LatexCommand citet
key "Chun-Keles-2007"

\end_inset

.
 The SPLS method combines the orthogonal decompositions of PLS with the
 sparsity of lasso-type variable selection.
 To do so, SPLS uses two tuning parameters: one that controls the number
 of orthogonal components and one that controls the lasso-type penalty.
 The optimal parameters are those that minimize the mean squared prediction
 error (MSEP) over a two-dimensional grid search.
 The MSEP is estimated by 10-fold cross-validation.
 SPLS was used for both selection-and-estimation (SPLS) and selection-only
 (SPLS-select).
\end_layout

\begin_layout Subsection
Implementation for beach regression
\end_layout

\begin_layout Standard
The response variable for our continuous regression models is the natural
 logarithm of the E.
 coli concentration.
 For the binary regression models, the response variable is an indicator
 of whether the concentration exceeds the regulatory threshold 
\begin_inset Formula $\delta=235$
\end_inset

 CFU/mL.Some pre-processing of the data was done.
 During pre-processing, some transformations were applied to the data.
 The beach water turbidity and the discharge of tributaries near each beach
 were log-transformed.
 Rainfall variables were all square-root transformed.
\end_layout

\begin_layout Standard
Include a table with pre/post processing discussion
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
Our assessment of the modeling techniques is based on their performance
 in predicting exceedances of the regulatory standard.
 Two types of cross validation was used to measure the performance in prediction
: leave-one-out (LOO) and leave-one-year-out (LOYO).
 In LOO CV, one observation is held out for validation while the rest of
 the data is used to train a model.
 The model is used to predict the concentration of that held out observation,
 and the process is repeated for each observation.
 Each cycle of LOYO CV holds out an entire year's worth of data for validation
 instead of a single observation.
 It is intended to approximate the performance of the modeling technique
 under a typical use case: a new model is estimated before the start of
 each annual beach season and then used for predicting exceedances during
 the season.
 The LOYO models in this study were estimated using all the available data
 except for the held out year, even that from future years.
 So for instance the 2012 models were estimated using the 2010-2011 and
 2013 data.
\end_layout

\begin_layout Standard
Some methods also used cross-validation internally to select tuning parameters.
 In those cases the internal CV was done using only the model data, and
 never looking at the held-out observation(s).
 This process is separate from - and does not affect - the CV to assess
 predictive performance.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
How did we evaluate the performance of each technique on all the different
 data sets
\end_layout

\begin_layout Itemize
AUROC: The area under the ROC (AUROC) curve assesses how accurately the
 predictions of the left-out observations are sorted.
 
\end_layout

\begin_layout Itemize
continuous variables using PRESS (skill --> Like Nash-Sutcliffe/R^2 over
 the fitted data) 
\end_layout

\begin_layout Itemize
True/False Positives/Negatives (needs a threshold)
\end_layout

\begin_layout Itemize
Which variables are selected for models where variable reduction takes place
\end_layout

\begin_deeper
\begin_layout Itemize
challenge regarding the fact that different variables are selected in each
 fold.
 Maybe use frequencies?
\end_layout

\begin_layout Itemize
also the number of variables selected (metric of complexity)
\end_layout

\end_deeper
\begin_layout Paragraph*
Area under the ROC curve
\end_layout

\begin_layout Standard
The receiver operating characteristic (ROC) curve is a graphical display
 of how well predictions are sorted.
 Each point on the curve represents the model's performance for a specific
 choice of the threshold 
\begin_inset Formula $\hat{\delta}$
\end_inset

 in terms of specificity and sensitivity.
 A model with well-sorted predictions is one where the predicted bacterial
 contentration (or the predicted probability of exeedance in the case of
 a binary regression model) is consistently greater when the true concentration
 exceeds the regulatory standard than when it does not.
 In such a case, the model may have both good sensitivity and good specificity
 over a wide range of thresholds 
\begin_inset Formula $\hat{\delta}$
\end_inset

.
\end_layout

\begin_layout Standard
The area under the ROC curve (AUROC) summarizes the model's performance
 over the range of possible thresholds.
 A model which perfectly separates exceedances from non-exceedances in predictio
n has an AUROC of one, while a model that predicts exceedances no better
 than a coin flip has an AUROC of 0.5.
\end_layout

\begin_layout Paragraph*
PRESS
\end_layout

\begin_layout Standard
While AUROC is concerned with how well the model's predictions separate
 exceedances from non-exceedances, the predictive residual error sum of
 squares (PRESS) measures how well a model's predictions match the observed
 bacterial concentration.
 The PRESS can only be computed for continuous regression methods.
 Let the model's predictions be denoted 
\begin_inset Formula $\tilde{y}_{i}$
\end_inset

 and letting the actual observed bacterial concentrations be denoted 
\begin_inset Formula $y_{i}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the total number of predictions.
 Then PRESS computed as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{PRESS}=\sum_{i=1}^{n}\left(\tilde{y}_{i}-y_{i}\right)^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
The PRESS statistic is of interest because a good model should accurately
 predict the bacterial concentration, and a model that more accurately predicts
 the concentration may be easier to trust.
 However, the AUROC is a more important as a metric of model performance
 than the PRESS because it directly measures the ability of a model to separate
 exceedances from non-exceedances.
 That said, we expect the two statistics to usually agree on which modeling
 methods are the best.
\end_layout

\begin_layout Paragraph*
True positives, etc.
\end_layout

\begin_layout Standard
While the AUROC is an important metric that should guide the choice of which
 modeling method to use, the real-world performance of any model for predicting
 exceedances will be measured not by its average performance over the possible
 range of thresholds, but by how well it distinguishes between exceedances
 and nonexceedances at the one specific threshold 
\begin_inset Formula $\hat{\delta}$
\end_inset

 that is used in the application.
 Because LOYO CV was used to simulate real-world application of the modeling
 methods, it seems natural to evaluate the models based on the accuracy
 of their decisions when provided with a realistic decision threshold 
\begin_inset Formula $\hat{\delta}$
\end_inset

.
\end_layout

\begin_layout Standard
Intuitively, 
\begin_inset Formula $\hat{\delta}$
\end_inset

 should adapt to the conditions that are observed in the beach's training
 data.
 If, for instance, exceedances are rare in the training data, then we expect
 few exceedances in the future, and should set the threshold 
\begin_inset Formula $\hat{\delta}$
\end_inset

 high to reflect this prior expectation.
 On the other hand, if the bacterial concentration often exceeds 
\begin_inset Formula $\delta$
\end_inset

, the threshold 
\begin_inset Formula $\hat{\delta}$
\end_inset

 should be set lower in order to properly flag more of those exceedances.
 This intuition was encoded into how 
\begin_inset Formula $\hat{\delta}$
\end_inset

 was set for the LOYO models.
 Specifically, 
\begin_inset Formula $\hat{\delta}$
\end_inset

 was set to the 
\begin_inset Formula $q$
\end_inset

th quantile of the fitted values for non-exceedances in the training set,
 where 
\begin_inset Formula $1-q$
\end_inset

 is the proportion of exceednaces in the training set.
 These LOYO predictions are summarized in the 2-by-2 tables of true positives,
 true negatives, false positives, and false negatives.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<compile-results, fig.cap='Mean ranks of the modeling techniques across
 the seven sites.
 At left are the mean ranks under leave-one-out cross validation, at the
 right are the mean ranks from leave-one-year-out cross validation.', fig.subcap=c
('LOO', 'LOYO'), out.width="0.49
\backslash

\backslash
linewidth", echo=FALSE, warning=FALSE, message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	require(dplyr)
\end_layout

\begin_layout Plain Layout

	require(reshape)
\end_layout

\begin_layout Plain Layout

	require(ggplot2)
\end_layout

\begin_layout Plain Layout

	require(brooks)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Load the raw results of the beauty contest:
\end_layout

\begin_layout Plain Layout

	setwd("..")
\end_layout

\begin_layout Plain Layout

	load("beauty_contest.RData")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	source("R/bootstrap-summarize.r")
\end_layout

\begin_layout Plain Layout

	source("R/bootstrap-summarize-annual.r")
\end_layout

\begin_layout Plain Layout

	source("R/get_annual_predictive_performance.r")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
AUROC
\end_layout

\begin_layout Standard
The methods are ranked at each site by AUROC and a mean rank (across sites)
 is computed for each method.
 The mean LOO and LOYO ranks are plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mean-ranks"

\end_inset

.
 The GBM and GBMCV techniques did best under both kinds of cross validation,
 and in both cases the adaptive lasso was the third-best technique.
 The PLS and GALM techniques both appeared well down the list of the best
 techniques.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<auroc-boxplot, fig.cap='Mean ranks of the modeling techniques across the
 seven sites.
 At left are the mean ranks under leave-one-out cross validation, at the
 right are the mean ranks from leave-one-year-out cross validation.', fig.subcap=c
('LOO', 'LOYO'), out.width="0.49
\backslash

\backslash
linewidth", echo=FALSE, warning=FALSE, message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	print(annual.auroc.boxplot)
\end_layout

\begin_layout Plain Layout

	print(LOO.auroc.boxplot)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
PRESS
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<press-boxplot, fig.cap='Mean ranks of the modeling techniques across the
 seven sites.
 At left are the mean ranks under leave-one-out cross validation, at the
 right are the mean ranks from leave-one-year-out cross validation.', fig.subcap=c
('LOO', 'LOYO'), out.width="0.49
\backslash

\backslash
linewidth", echo=FALSE, warning=FALSE, message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	print(annual.press.boxplot)
\end_layout

\begin_layout Plain Layout

	print(LOO.press.boxplot)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
True positives, etc.
\end_layout

\begin_layout Standard
The performance of GBM was about equivalent to that of GBMCV at a much lower
 computational cost, so GBMCV will hereafter be set aside for discussion
 in favor of GBM.
 Further analysis focuses on GBM and the adaptive lasso.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<counts-barcharts, fig.cap='Mean ranks of the modeling techniques across
 the seven sites.
 At left are the mean ranks under leave-one-out cross validation, at the
 right are the mean ranks from leave-one-year-out cross validation.', fig.subcap=c
('', 'LOYO'), out.width="0.32
\backslash

\backslash
linewidth", echo=FALSE, warning=FALSE, message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	multiplot(plotlist=pp, cols=1)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
In general, the GBM, and GBMCV, and AL techniques produced comparable results
 that were superior to the other techniques in terms of predictive performance.
 Since the GBMCV models take much longer to compute than the others, we
 will not include them in our more detailed analysis of the modeling results.
\end_layout

\begin_layout Standard
Which type of model is generally the best?
\end_layout

\begin_layout Standard
Under what conditions do some outperform others?
\end_layout

\begin_layout Standard
Relative value of overall best model versus methods that help trim variables?
 e.g.
 how valuable is it to reduce number of predictors? Further, which variables
 get cut? Expensive ones? Cheap ones?
\end_layout

\begin_layout Standard
How important is computational expense? Only an issue for model fitting
 --- not prediction, but worth quantifying.
 E.g.
 if GBM with cross validation takes hours, how much better? 
\end_layout

\begin_layout Standard
Model tuning for GBM versus GBM-CV --> notes on how GBM is faster with similar
 performance (e.g.
 CV is overkill maybe)
\end_layout

\begin_layout Section
Acknowledgments
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "C:/Users/wrbrooks/git/beauty_contest/references/beautycontest"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\end_body
\end_document
