#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Beauty Contest
\end_layout

\begin_layout Abstract
Pithy, concise and informative.
 May bring the reader to tears due to the beauty of it.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The availability of large data sets for building regression models to predict
 the bacterial counts in beach water is both an opportunity and a challenge.
 
\end_layout

\begin_layout Subsection
Data Sources
\end_layout

\begin_layout Standard

\emph on
Possibly move this to the end of the section
\end_layout

\begin_layout Standard
Which sites
\end_layout

\begin_layout Standard
Where are they
\end_layout

\begin_layout Standard
What specific sources sources of data (plug EnDDAT)
\end_layout

\begin_layout Standard
Will include a map and tables
\end_layout

\begin_layout Subsection
Listing of specific statistical techniques
\end_layout

\begin_layout Standard
Fourteen different regression modeling techniques were considered.
 Each technique uses one of five modeling algorithms: GBM, the adaptive
 lasso, the genetic algorithm, PLS, or sparse PLS.
 Each technique is aplied to either continuous or binary regression and
 to either modeling, or variable selection only.
\end_layout

\begin_layout Subsubsection*
Continuous or binary regression
\end_layout

\begin_layout Standard
The goal of predicting exceednaces of the water quality standard was approached
 in two ways: one was to predict the bacterial concentration and then compre
 the prediction to a threshold.
 The other was to predict the state of a binary indicator, which is coded
 as zero when the concetration is below the standard and one when the concentrat
ion exceeds the standard.
 Techniques taking the former approach are continuous regression techniques,
 those taking the latter approach are binary regression techniques.
\end_layout

\begin_layout Subsubsection*
Modeling or selection only
\end_layout

\begin_layout Standard
Some methods are labeled 
\begin_inset Quotes eld
\end_inset

select
\begin_inset Quotes erd
\end_inset

, which means that they are used for variable selection only.
 In these cases, once the variables are selected, the regression model is
 estimated using ordinary least squares for the continuous regression techniques
, or ordinary logistic regression for the binary regression techniques.
\end_layout

\begin_layout Subsubsection
GBM
\end_layout

\begin_layout Standard
GBM refers to the gradient boosting machine (GBM) of 
\begin_inset CommandInset citation
LatexCommand citet
key "Friedman-2001"

\end_inset

.
 A GBM model is a so-called random forest model - a collection of many regressio
n trees.
 Prediction is done by averaging the outputs of the trees.
 Two GBM-based techniques are explored - we refer to them as GBM and GBMCV.
 The difference is that GBMCV selects the number of trees in a model using
 cross-validation (CV), while GBM uses the so-called out-of-bag (OOB) error
 estimate for the same purpose.
 The CV method is slower but should tend to give a more accurate estimate
 of the prediction error.
 The parameters shared by all GBM models in this study are:
\end_layout

\begin_layout Standard
Number of trees: 10000
\end_layout

\begin_layout Standard
Shrinkage parameter: 0.0005
\end_layout

\begin_layout Standard
Minimum observations per node: 5
\end_layout

\begin_layout Standard
Depth of each tree: 5
\end_layout

\begin_layout Standard
Bagging fraction: 0.5
\end_layout

\begin_layout Standard
One key decision when fitting a GBM model is how many regression trees to
 use.
 Our strategy is to produce a model using a very large number of trees,
 and then to prune back to the optimal number.
 The optimal number of trees is the number than minimizes our estimate of
 the prediction error.
\end_layout

\begin_layout Subsubsection
Adaptive Lasso
\end_layout

\begin_layout Standard
The adaptive lasso 
\begin_inset CommandInset citation
LatexCommand citet
key "Zou-2006"

\end_inset

 is a regression method that simultaneously selects relevant predictors
 and estimates their coefficients by adding a penalty to the sum of the
 squared residuals.
 For linear regression, the adaptive lasso estimates 
\begin_inset Formula $\hat{{\bm{{\beta}}}}$
\end_inset

 minimize the criterion 
\begin_inset Formula $\sum_{i=1}^{n}(y_{i}-X_{i}\beta)^{2}+\lambda\sum_{j=1}^{p}\frac{{|\beta_{j}|}}{\tilde{{|\beta_{j}|^{\gamma}}}}$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is a tuning parameter and 
\begin_inset Formula $\tilde{{\bm{{\beta}}}}$
\end_inset

 is a consistent estimate of the regression coefficients.
\end_layout

\begin_layout Standard
In this work, 
\begin_inset Formula $\gamma$
\end_inset

 is set to one and the adaptive lasso tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 is selected to minimize the AICc of 
\begin_inset CommandInset citation
LatexCommand citet
key "Hurvich-Simonoff-Tsai-1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Genetic algorithm
\end_layout

\begin_layout Standard
The genetic algorithm 
\begin_inset CommandInset citation
LatexCommand citep
key "Fogel-1998"

\end_inset

 is a variable-selection method that works by analogy to natural selection,
 where so-called chromosomes represent regression models.
 A variable is included in the model if the corresponding element of the
 chrmosome is one, but not otherwise.
 Chromosomes are produced in successive generations, where the first generation
 is produced randomly and subsequent generations are produced by combining
 chromosomes from the current generation, with additional random drift.
 The chance that a chromosome in the current generation will produce offspring
 in the next generation is a determined by its fitness.
 The fitness of each chromosome is calculated by the corrected Akaike Informatio
n Criterion (AICc) 
\begin_inset CommandInset citation
LatexCommand citet
key "Akaike-1973,Hurvich-Tsai-1989"

\end_inset

.
\end_layout

\begin_layout Standard
Our implementation used 100 generations and each generation contained 200
 chromosomes.
 
\end_layout

\begin_layout Subsubsection
PLS
\end_layout

\begin_layout Standard
Partial least squares (PLS) regression is a tool for building regression
 models with many covariates.
 PLS works by decomposing the covariates into mutually orthogonal components,
 with the components then used as the variables in a regression model.
 This is similar to principal components regression (PCR), but the way PLS
 components are chosen ensures that they are aligned with the model output.
 On the other hand, PCR is sometimes criticised for decomposing the covariates
 into components that are unrelated to the model's output.
\end_layout

\begin_layout Standard
To use PLS, one must decide how many components to use in the model.
 The code for this paper follows the method described in (Brooks-Fienen-Corsi-20
12), using the PRESS statistic to select the number of components.
\end_layout

\begin_layout Subsubsection
SPLS
\end_layout

\begin_layout Standard
Sparse PLS (SPLS) is introduced in 
\begin_inset CommandInset citation
LatexCommand citet
key "Chun-Keles-2007"

\end_inset

.
 The SPLS method combines the orthogonal decompositions of PLS with the
 sparsity of lasso-type variable selection.
 To do so, SPLS uses two tuning parameters: one that controls the number
 of orthogonal components and one that controls the lasso-type penalty.
 In this work, the optimal tuning parameters are those that minimize an
 estimate of the prediction error that is based on cross-validation.
 The optimal tuning parameters are located via a two-dimensional grid search.
\end_layout

\begin_layout Subsection
Implementation for beach regression
\end_layout

\begin_layout Standard
The response variable for our continuous regression models is the natural
 logarithm of the E.
 coli concentration.
 For the binary regression models, the response variable is 
\end_layout

\begin_layout Standard
Include a table with pre/post processing discussion
\end_layout

\begin_layout Standard
This includes tuning of parameters
\end_layout

\begin_layout Standard
Some specific data issues because we are estimating a threshold exceedence
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
Modeling methods were assessed by their performance in cross-validation.
 Two separate assesments were made: leave-one-out and leave-one-year-out.
\end_layout

\begin_layout Standard
Leave one year out cross-validaition was used to simulate how the model
 would have done in a setting where a new model was built each year and
 used for predicting exceedances during that year's beach season.
 Becaues only a few years of data were available, we did not limit ourselves
 to building models from what data was available prior to each year.
 For example, the model we used for predicting the exceedances during the
 2012 beach season was trained on data from the 2009-2011 and 2013 beach
 seasons.
 
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
How did we evaluate the performance of each technique on all the different
 data sets
\end_layout

\begin_layout Itemize
for all cases ---> AUC (ROC curve)
\end_layout

\begin_layout Itemize
continuous variables using PRESS (skill --> Like Nash-Sutcliffe/R^2 over
 the fitted data) 
\end_layout

\begin_layout Itemize
True/False Positives/Negatives (needs a threshold)
\end_layout

\begin_layout Itemize
Which variables are selected for models where variable reduction takes place
\end_layout

\begin_deeper
\begin_layout Itemize
challenge regarding the fact that different variables are selected in each
 fold.
 Maybe use frequencies?
\end_layout

\begin_layout Itemize
also the number of variables selected (metric of complexity)
\end_layout

\end_deeper
\begin_layout Paragraph*
OPTIONAL
\end_layout

\begin_layout Itemize
AIC/BIC? --> not the same model in each fold so maybe not possible
\end_layout

\begin_layout Itemize
Maybe some form of confusion matrices -- perhaps a grid of them with or
 without companion variance plots or other estimates of the range of results
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Itemize
Performance over prediction from cross validation
\end_layout

\begin_layout Itemize
Maybe anecdotal showing fit over data set
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
In general, the AL, GALM, GBM, and GBMCV techniques produced comparable
 results that were superior to the other techniques in terms of predictive
 performance.
 Since the GBMCV models take much longer to computer than the others, we
 will not include them in our more detailed analysis of the modeling results.
\end_layout

\begin_layout Standard
Which type of model is generally the best?
\end_layout

\begin_layout Standard
Under what conditions do some outperform others?
\end_layout

\begin_layout Standard
Relative value of overall best model versus methods that help trim variables?
 e.g.
 how valuable is it to reduce number of predictors? Further, which variables
 get cut? Expensive ones? Cheap ones?
\end_layout

\begin_layout Standard
How important is computational expense? Only an issue for model fitting
 --- not prediction, but worth quantifying.
 E.g.
 if GBM with cross validation takes hours, how much better? 
\end_layout

\begin_layout Standard
Model tuning for GBM versus GBM-CV --> notes on how GBM is faster with similar
 performance (e.g.
 CV is overkill maybe)
\end_layout

\begin_layout Section
Acknowledgments
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/wesley/git/beauty_contest/references/beautycontest"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\end_body
\end_document
